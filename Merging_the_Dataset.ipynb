{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEUIsKeA+vQ/gXEOVdLh+s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShabnaIlmi/SpamSense-AI/blob/main/Merging_the_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing the Relevant Libaries**"
      ],
      "metadata": {
        "id": "jDJwDEa2tiHa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8NXzzZertU_D"
      },
      "outputs": [],
      "source": [
        "# Importing the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mounting the Google Drive**"
      ],
      "metadata": {
        "id": "QtbbE_1gtp__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "W1rgHuJVtupw",
        "outputId": "56bca914-ee38-4526-b2b3-cf1ce5d2d1dc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-866a2c8036b9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Mounting the google drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading the Datasets**"
      ],
      "metadata": {
        "id": "5gS9xZcEty0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the file path\n",
        "file_path_true = \"/content/drive/MyDrive/Multi-Type-Spam-Detection/Dataset/True_Cleaned.csv\"\n",
        "\n",
        "# Loading the dataset with the correct delimiter (semicolon)\n",
        "data_true = pd.read_csv(file_path_true, encoding=\"utf-8\")"
      ],
      "metadata": {
        "id": "QXhfX062t2H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the file path\n",
        "file_path_false = \"/content/drive/MyDrive/Multi-Type-Spam-Detection/Dataset/Fake_Cleaned.csv\"\n",
        "\n",
        "# Loading the dataset with the correct delimiter (semicolon)\n",
        "data_fake = pd.read_csv(file_path_false, encoding=\"utf-8\")"
      ],
      "metadata": {
        "id": "QjUtruHDuJZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the datasets head\n",
        "data_true.head()"
      ],
      "metadata": {
        "id": "K10UvMRDuAba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_fake.head()"
      ],
      "metadata": {
        "id": "cW9mps0BvGRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging both the Datasets\n",
        "data = pd.concat([data_true, data_fake], axis=0)"
      ],
      "metadata": {
        "id": "kiwj_Y6axuR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying Dataset Information\n",
        "data.info()"
      ],
      "metadata": {
        "id": "VVSBjptwxymu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploratatry Data Analysis**"
      ],
      "metadata": {
        "id": "I_RAq8aS424f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Identifying Categorical and Numerical Columns**"
      ],
      "metadata": {
        "id": "NNeBHHar47ok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identifying Categorical and Numerical Columns\n",
        "categorical_columns = data.select_dtypes(include=['object']).columns\n",
        "numerical_columns = data.select_dtypes(include=['int64', 'float64']).columns"
      ],
      "metadata": {
        "id": "8wOiLGAJ49-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Categorical Features**"
      ],
      "metadata": {
        "id": "80VVbcDa5ElI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the Categorical Features\n",
        "print(\"\\nCategorical Features:\\n\")\n",
        "print(categorical_columns)"
      ],
      "metadata": {
        "id": "1wlLfG1C5HFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unique Values and Their Counts Relevant to Each categorical Feature**"
      ],
      "metadata": {
        "id": "haaQMbdh5LPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the Unique Values and Their Counts Relevant to Each Categorical Column\n",
        "print(\"Unique values and their count relevant to each categorical feature:\\n\")\n",
        "for column in categorical_columns:\n",
        "    unique_values = data[column].unique()\n",
        "    value_counts = data[column].value_counts()\n",
        "    print(value_counts)\n",
        "    print(\" \")"
      ],
      "metadata": {
        "id": "65mLX3he5Krm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Numerical Features**"
      ],
      "metadata": {
        "id": "b-ModBN45SUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the Numerical Features\n",
        "print(\"\\nNumerical Features:\\n\")\n",
        "print(numerical_columns)"
      ],
      "metadata": {
        "id": "TBN4wfTW5Vof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visualization of the Target Variable**"
      ],
      "metadata": {
        "id": "_UmO4MlF5ZsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization of the Distribution of the Target Variable\n",
        "sns.countplot(data=data, x='status')\n",
        "plt.title('Target Variable Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-AXp1N6r5bxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**"
      ],
      "metadata": {
        "id": "kSIBRaNl4WnI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling the null values in the dataset.**"
      ],
      "metadata": {
        "id": "D5XBhHKW4buX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Dropping the null values in the text column\n",
        "data.dropna(subset=['text'], inplace=True)"
      ],
      "metadata": {
        "id": "7PtHSxEg5iDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Dropping the 'subject' column\n",
        "data.drop(columns=['subject'], inplace=True)"
      ],
      "metadata": {
        "id": "R0LcOb1CAX0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying Dataset Information\n",
        "data.info()"
      ],
      "metadata": {
        "id": "WFGCQbQZ5vsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Categorical Columns**"
      ],
      "metadata": {
        "id": "_xHsGaW_6BWI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Encoding**"
      ],
      "metadata": {
        "id": "7Hs0TAlq6EGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applying Label Encoding for the\n",
        "Categorical Columns**"
      ],
      "metadata": {
        "id": "bw7kXaln6IbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Encoding the Categorical Column\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Google Drive Path\n",
        "save_path = \"/content/drive/MyDrive/Multi-Type-Spam-Detection/Encoders/\"\n",
        "\n",
        "# Listing the columns for encoding\n",
        "encoding_columns = ['status']\n",
        "\n",
        "# Encoding the categorical features\n",
        "label_encoders = {}\n",
        "\n",
        "for feature in encoding_columns:\n",
        "    label_encoder = LabelEncoder()\n",
        "    data[feature] = label_encoder.fit_transform(data[feature])\n",
        "    label_encoders[feature] = label_encoder\n",
        "\n",
        "# Check if directory exists, if not, create it\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "# Save the encoders\n",
        "encoder_file = os.path.join(save_path, 'label_encoders.pkl')\n",
        "with open(encoder_file, 'wb') as file:\n",
        "    pickle.dump(label_encoders, file)\n",
        "\n",
        "print(f\"Label Encoding Applied and Encoders Saved Successfully at: {encoder_file} 🎯\")"
      ],
      "metadata": {
        "id": "eYY2Og-B6NXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF for 'title'\n",
        "tfidf_title = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1,2))\n",
        "df_title_tfidf = tfidf_title.fit_transform(df['title'])\n",
        "\n",
        "# Save the title vectorizer\n",
        "with open(\"/content/drive/MyDrive/Multi-Type-Spam-Detection/Encoders/tfidf_title.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tfidf_title, f)\n",
        "\n",
        "# TF-IDF for 'text'\n",
        "tfidf_text = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1,2))\n",
        "df_text_tfidf = tfidf_text.fit_transform(df['text'])\n",
        "\n",
        "# Save the text vectorizer\n",
        "with open(\"/content/drive/MyDrive/Multi-Type-Spam-Detection/Encoders/tfidf_text.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tfidf_text, f)"
      ],
      "metadata": {
        "id": "8bqqccNWh6zT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying dataset Information\n",
        "data.info()"
      ],
      "metadata": {
        "id": "Gul86t--6_bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Splitting the Target Variables and the Features\n",
        "X = data.drop(columns=['status'])\n",
        "y = data['status']"
      ],
      "metadata": {
        "id": "kzwD-YGU7IMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Standardizing the Features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "5eUs4g0s8bKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Splitting the Training and the Testing the Datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "vpMvPaZe79c5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The Target Variable**"
      ],
      "metadata": {
        "id": "aNkGMGDN7cHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the Distribution of the 'status' Variable\n",
        "y.value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "VTwa6z7V7m-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the Distribution of the Target Variable\n",
        "sns.countplot(data=data, x='status')\n",
        "plt.title('Target Variable Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tI2DFf-P7hmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building the Model**"
      ],
      "metadata": {
        "id": "CycHdCtv8poQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Based on Gradient Boost**"
      ],
      "metadata": {
        "id": "0X6b1I5v8sQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter Tuning with GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.05, 0.1, 0.2],\n",
        "    'subsample': [0.8, 1],\n",
        "    'colsample_bytree': [0.8, 1]\n",
        "}"
      ],
      "metadata": {
        "id": "C5uSVt8G93Fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb = XGBClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "xzESSkxk95Hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "best_model = grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "h_Xzp1mJ990P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-Validation Score\n",
        "cv_score = cross_val_score(best_model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "print(f\"Cross-Validation Score: {np.mean(cv_score):.4f}\")"
      ],
      "metadata": {
        "id": "qkf7lwnd9__u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Training\n",
        "best_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "qU6SC46w-DZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "y_pred = best_model.predict(X_test)"
      ],
      "metadata": {
        "id": "-CEbexYv-J3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation Metrics\n",
        "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "HVaTOQDP-MFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6zqFn_3r-PeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Importance Visualization\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(data.columns[:-1], best_model.feature_importances_)\n",
        "plt.title(\"Feature Importance\")\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.ylabel(\"Features\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DLTopeiQ-ScM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Saving the Model\n",
        "joblib.dump(best_model, '/content/drive/MyDrive/Multi-Type-Spam-Detection/Models/news_article_model.pkl')\n",
        "print(\"Model Saved Successfully ✅\")\n",
        "\n",
        "# Saving the Scaler (if you're using StandardScaler or MinMaxScaler)\n",
        "joblib.dump(scaler, '/content/drive/MyDrive/Multi-Type-Spam-Detection/Scaler/news_article_scaler.pkl')\n",
        "print(\"Scaler Saved Successfully ✅\")"
      ],
      "metadata": {
        "id": "vI3bDhP6-oJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the Model\n",
        "best_model_loaded = joblib.load('/content/drive/MyDrive/Multi-Type-Spam-Detection/Encoders/news_article_model.pkl')\n",
        "print(\"Model Loaded Successfully ✅\")\n",
        "\n",
        "# Loading the Scaler\n",
        "scaler_loaded = joblib.load('/content/drive/MyDrive/Multi-Type-Spam-Detection/Scaler/news_article_scaler.pkl')\n",
        "print(\"Scaler Loaded Successfully ✅\")"
      ],
      "metadata": {
        "id": "TMcsbafmAQGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Load the saved model, scaler, and encoders\n",
        "tfidf_title_path = '/content/drive/MyDrive/Multi-Type-Spam-Detection/Encoders/tfidf_title.pkl'\n",
        "tfidf_text_path = '/content/drive/MyDrive/Multi-Type-Spam-Detection/Encoders/tfidf_text.pkl'\n",
        "encoder_path = '/content/drive/MyDrive/Multi-Type-Spam-Detection/Encoders/label_encoder.pkl'\n",
        "\n",
        "# Load the trained model and scaler\n",
        "model = joblib.load(model_path)\n",
        "scaler = joblib.load(scaler_path)\n",
        "\n",
        "# Load the saved TF-IDF vectorizers\n",
        "with open(tfidf_title_path, 'rb') as f:\n",
        "    tfidf_title = pickle.load(f)\n",
        "\n",
        "with open(tfidf_text_path, 'rb') as f:\n",
        "    tfidf_text = pickle.load(f)\n",
        "\n",
        "# Load the label encoder for 'status'\n",
        "with open(encoder_path, 'rb') as f:\n",
        "    label_encoders = pickle.load(f)\n",
        "status_encoder = label_encoders['status']\n",
        "\n",
        "# Preprocessing function (apply saved transformations)\n",
        "def preprocess_input(title, text, status):\n",
        "    # Transform text data using the saved TF-IDF vectorizers\n",
        "    title_tfidf = tfidf_title.transform([title]).toarray()\n",
        "    text_tfidf = tfidf_text.transform([text]).toarray()\n",
        "\n",
        "    # Encode categorical 'status' feature using saved label encoder\n",
        "    status_encoded = status_encoder.transform([status])[0]\n",
        "\n",
        "    # Combine features\n",
        "    input_features = np.hstack([title_tfidf, text_tfidf, [status_encoded]])\n",
        "\n",
        "    # Apply the scaler\n",
        "    input_features = scaler.transform(input_features.reshape(1, -1))\n",
        "\n",
        "    return input_features\n",
        "\n",
        "# Get user input\n",
        "title = input(\"Enter the title of the news article: \")\n",
        "text = input(\"Enter the text of the news article: \")\n",
        "status = input(\"Enter the status of the article (e.g., published, draft, etc.): \")\n",
        "\n",
        "# Preprocess the input\n",
        "processed_input = preprocess_input(title, text, status)\n",
        "\n",
        "# Make the prediction using the trained model\n",
        "prediction = model.predict(processed_input)\n",
        "\n",
        "# Output the result\n",
        "print(\"This article is SPAM.\" if prediction == 1 else \"This article is NOT spam.\")"
      ],
      "metadata": {
        "id": "iCN4wVlpC-wl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}